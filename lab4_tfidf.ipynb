{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the List of Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching we receive a huge list of results, then we have to rank the results and return the most informative!\n",
    "\n",
    "## Number of overlapping words:\n",
    "- not normalized by length of document\n",
    "\n",
    "## Jaccard Coefficient\n",
    "- $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories:  ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Housing articles: ['test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/11170', 'training/11665', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/9615']\n",
      "\n",
      "Words in an arbitrary article: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "# NLTK supports access to different datasets https://www.nltk.org/book/ch02.html\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "print(\"\\nCategories: \", reuters.categories())\n",
    "\n",
    "housing_articles = reuters.fileids('housing')\n",
    "print(\"\\nHousing articles:\", housing_articles)\n",
    "\n",
    "print(\"\\nWords in an arbitrary article:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "Long Document words number: 131\n",
      "Long Document overlap: 3\n",
      "Long Document JC: 0.03571428571428571\n",
      "\n",
      "Short Document words number: 7\n",
      "Short Document overlap: 3\n",
      "Short Document JC: 0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def word_overlap(doc_tokens, query_tokens):\n",
    "    return sum([1 for _tok in query_tokens if _tok in doc_tokens])\n",
    "\n",
    "def jaccard_coeff(doc_tokens, query_tokens):\n",
    "    # naive intersection of sets\n",
    "    return len(set(doc_tokens).intersection(set(query_tokens))) / len(set(doc_tokens).union(set(query_tokens)))\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"Long Document words number:\", len(reuters.words('training/6067')))\n",
    "print(\"Long Document overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Long Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document words number:\", len(short_similar_document))\n",
    "print(\"Short Document overlap:\", word_overlap(query_tokens, short_similar_document))\n",
    "print(\"Short Document JC:\", jaccard_coeff(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__But we also want to__:\n",
    "- Give _more weight_ to _less frequent words_ in the documents - __Balridge, prices__\n",
    "- Give _less weight_ to _more frequent words_ in the documents - __how, much, housing, to__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document word overlap: 13\n",
      "Document JC: 0.06593406593406594\n",
      "Document Content: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "query_tokens = tokenizer.tokenize('how much will the housing go up in the next month according to Balridge?')\n",
    "print(\"Document word overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document Content:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Term Frequency- Inverted Document Frequency\n",
    "- View documents as __Bags Of Words__\n",
    "- Mary lent John some money. = John lent Mary some money.\n",
    "- Formula: \n",
    "\n",
    "$$TF * IDF (word, document) = (1+log(tf(word, document)) * log(\\frac{n}{df(word)})$$\n",
    "- n - total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "- __Frequency of word in a document (here, raw count)__\n",
    "- __0 if the term is not met in the document!!!__\n",
    "- Relevance does not increase proportionally with frequency -> __log (base of 10)__\n",
    "- Makes TF-IDF __increase with the number of occurrences__ within a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>34</td>\n",
       "      <td>2.531479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>31</td>\n",
       "      <td>2.491362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>21</td>\n",
       "      <td>2.322219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pct</td>\n",
       "      <td>15</td>\n",
       "      <td>2.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>September</td>\n",
       "      <td>11</td>\n",
       "      <td>2.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>units</td>\n",
       "      <td>11</td>\n",
       "      <td>2.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>starts</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>August</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  freq        tf\n",
       "0          .    34  2.531479\n",
       "1          ,    31  2.491362\n",
       "2         in    21  2.322219\n",
       "3        pct    15  2.176091\n",
       "4          1    13  2.113943\n",
       "5        000    13  2.113943\n",
       "6  September    11  2.041393\n",
       "7      units    11  2.041393\n",
       "8     starts    10  2.000000\n",
       "9     August    10  2.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>below</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>level</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>687</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  freq   tf\n",
       "110    689     1  1.0\n",
       "111     11     1  1.0\n",
       "112  below     1  1.0\n",
       "113  level     1  1.0\n",
       "114    687     1  1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pandas\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(Counter(reuters.words('test/20116')).most_common(), columns=['token', 'freq'])\n",
    "df['tf'] = 1 + np.log10(df['freq'])\n",
    "df.head(10)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency\n",
    "- __Number of documents containing the word__ - an inversed measure of significance\n",
    "- Logarithm with base 10 dampens the effect of IDF\n",
    "- Affects ranking of queries with __at least 2 terms__\n",
    "- Makes TFIDF __increase with the rarity of the term in the collection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>to</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>U</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  doc_freq       idf\n",
       "10     .        20  0.000000\n",
       "36     ,        19  0.022276\n",
       "139  the        17  0.070581\n",
       "145   to        17  0.070581\n",
       "186    U        16  0.096910"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>598</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>503</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>association</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  doc_freq      idf\n",
       "398          598         1  1.30103\n",
       "399          111         1  1.30103\n",
       "401          503         1  1.30103\n",
       "367          150         1  1.30103\n",
       "832  association         1  1.30103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "for fileid in housing_articles:\n",
    "    for _word in set(reuters.words(fileid)):\n",
    "        document_frequency[_word] += 1\n",
    "\n",
    "idf_df = pd.DataFrame(list(document_frequency.items()), columns=['word', 'doc_freq'])\n",
    "idf_df['idf'] = np.log10(len(housing_articles)/idf_df['doc_freq'])\n",
    "idf_df.sort_values(by=['idf'], inplace=True)\n",
    "idf_df.head()\n",
    "idf_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate score for a document D w.r.t. a query Q, __summing over tfidf scores of the words in both D and Q (intersection)__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/6067')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x0000014A9CA32980>, {'Britain': 3, '33': 1, 'man': 2, 'at': 2, '-': 2, 'Edgley': 2, ',': 1, 'Ross': 2, 'to': 3, 'first': 1, 'swim': 2, 'around': 2, 'Circumnavigate': 1, 'Spent': 1, '5': 1, 'Months': 1, 'Sea': 1, 'Swimming': 1, 'this': 1, '!': 1, 'Get': 1, '4': 1, 'Set': 1, '?': 1, 'Can': 1, 'H2OMG': 1, 'British': 1, 'GQ': 1, 'swimming': 1, 'world': 1, '|': 1, 'strongman': 1, 'of': 1, 'Welcome': 1, 'the': 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Britain</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>to</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>around</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ross</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Edgley</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  doc_freq       idf\n",
       "0   Britain         3  0.823909\n",
       "8        to         3  0.823909\n",
       "11   around         2  1.000000\n",
       "7      Ross         2  1.000000\n",
       "5    Edgley         2  1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>first</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Get</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>the</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  doc_freq      idf\n",
       "9   first         1  1.30103\n",
       "6       ,         1  1.30103\n",
       "1      33         1  1.30103\n",
       "20    Get         1  1.30103\n",
       "34    the         1  1.30103"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    'Ross Edgley, at 33 - first man to swim around Britain',\n",
    "    'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea',\n",
    "    'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?',\n",
    "    'Welcome to the world of strongman swimming | British GQ'\n",
    "]\n",
    "\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "for doc_tkns in tokenized_docs:\n",
    "    for _word in set(doc_tkns):\n",
    "        document_frequency[_word] += 1\n",
    "print(document_frequency)\n",
    "\n",
    "idf_df = pd.DataFrame(list(document_frequency.items()), columns=['word', 'doc_freq'])\n",
    "idf_df['idf'] = np.log10(len(housing_articles)/idf_df['doc_freq'])\n",
    "idf_df.sort_values(by=['idf'], inplace=True)\n",
    "idf_df.head()\n",
    "idf_df.tail()\n",
    "\n",
    "def tfidf_score(query_tokens, document_tokens):\n",
    "    def tfidf(word):\n",
    "        counter = Counter(document_tokens)\n",
    "        term_cnt = counter[word]\n",
    "        return (1 + np.log10(term_cnt)) * idf_df[idf_df['word']==word].iloc[0]['idf']\n",
    "    \n",
    "    overlapping_tokens = set(query_tokens).intersection(set(document_tokens))\n",
    "    return sum([tfidf(_word) for _word in overlapping_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/6067').count(\"who\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query: 'Who was the first man ever to swim around Britain?'\n",
    "- Doc1: 'Ross Edgley, at 33 - first man to swim around Britain'\n",
    "- Doc2: 'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea'\n",
    "- Doc3: 'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?'\n",
    "- Doc4: 'Welcome to the world of strongman swimming | British GQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.948847477552619"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.6478174818886375"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5.1249387366083"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.1249387366083"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = 'Who was the first man ever to swim around Britain?'\n",
    "\n",
    "#query = 'Who was the first man ever to to swim around Britain?'\n",
    "\n",
    "tokens_query = tokenizer.tokenize(query)\n",
    "\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "\n",
    "\n",
    "for doc in tokenized_docs:\n",
    "    tfidf_score(tokens_query, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!TODO: Bring the solution back into the notebook!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "- Each document can be represented by a vector, where the terms are the axes of the space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Ross Edgley, at 33 33 - first man to swim around Britain',\n",
    "    'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea',\n",
    "    'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?',\n",
    "    'Welcome to the world of strongman swimming | British GQ'\n",
    "]\n",
    "query = 'Who was the first man ever to swim around Britain?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_extraction.text:\n",
    "- __CountVectorizer__ - Convert a collection of text documents to a matrix of token counts.\n",
    "- __TfidfVectorizer__ - Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "- Two main methods __fit__ and __transform__ :\n",
    "    - __fit__ goes through the provided documents and __collects the vocabulary__\n",
    "    - __transform__ transforms __documents in text representation to a vector representation__ according to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update example to have word with higher frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m      4\u001b[0m count_vectorizer\u001b[38;5;241m.\u001b[39mfit(documents)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(count_vectorizer.vocabulary_) # word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform produces a sparse representations of documents - only values != 0\n",
    "# we need toarray() to preview the whole lists\n",
    "count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39090017 0.30819018 0.30819018 0.24950651 0.         0.\n",
      "  0.         0.30819018 0.39090017 0.         0.         0.\n",
      "  0.30819018 0.         0.         0.30819018 0.         0.\n",
      "  0.         0.         0.30819018 0.         0.         0.\n",
      "  0.24950651 0.         0.        ]\n",
      " [0.         0.         0.30505473 0.24696809 0.         0.\n",
      "  0.38692324 0.30505473 0.         0.         0.         0.\n",
      "  0.         0.38692324 0.         0.30505473 0.38692324 0.\n",
      "  0.38692324 0.         0.         0.         0.         0.\n",
      "  0.24696809 0.         0.        ]\n",
      " [0.         0.28061469 0.         0.22718178 0.         0.35592415\n",
      "  0.         0.         0.         0.35592415 0.         0.35592415\n",
      "  0.28061469 0.         0.         0.         0.         0.35592415\n",
      "  0.         0.         0.28061469 0.28061469 0.         0.35592415\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.35291425 0.\n",
      "  0.         0.         0.         0.         0.35291425 0.\n",
      "  0.         0.         0.35291425 0.         0.         0.\n",
      "  0.         0.35291425 0.         0.27824164 0.35291425 0.\n",
      "  0.22526059 0.35291425 0.35291425]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ross': 15,\n",
       " 'edgley': 7,\n",
       " 'at': 2,\n",
       " '33': 0,\n",
       " 'first': 8,\n",
       " 'man': 12,\n",
       " 'to': 24,\n",
       " 'swim': 20,\n",
       " 'around': 1,\n",
       " 'britain': 3,\n",
       " 'circumnavigate': 6,\n",
       " 'spent': 18,\n",
       " 'months': 13,\n",
       " 'sea': 16,\n",
       " 'get': 9,\n",
       " 'set': 17,\n",
       " 'swimming': 21,\n",
       " 'h2omg': 11,\n",
       " 'can': 5,\n",
       " 'this': 23,\n",
       " 'welcome': 25,\n",
       " 'the': 22,\n",
       " 'world': 26,\n",
       " 'of': 14,\n",
       " 'strongman': 19,\n",
       " 'british': 4,\n",
       " 'gq': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "print(tfidf_vectorizer.fit_transform(documents).toarray())\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing vector similarities\n",
    "- We would like to find documents close to a given document or the closest documents to a query\n",
    "- __Euclidean distance__? - shorter documents will be closer to each other rather than documents talking about same topic\n",
    "- __Cosine similarity__ of the angle between two documents\n",
    "    - divide each vector by its norm to achieve __unit length vectors__\n",
    "    - cosine similarity is simply the __dot product__ of two unit length vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cosine SImilarity](img/cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array([1, 0, 0, 1, 2])\n",
    "vector2 = np.array([0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40824829, 0.        , 0.        , 0.40824829, 0.81649658]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.40824829, 0.        , 0.        , 0.40824829, 0.81649658])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "preprocessing.normalize([vector1], norm='l2')\n",
    "vector1 / np.sqrt(sum(vector1**2))\n",
    "\n",
    "unit_vector1 = preprocessing.normalize([vector1], norm='l2')[0]\n",
    "unit_vector2 = preprocessing.normalize([vector2], norm='l2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(unit_vector1, unit_vector2)\n",
    "sum([unit_vector1[i]*unit_vector2[i] for i in range(len(unit_vector1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([vector1, vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : calculate the closes document to the query from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the first man ever to swim around Britain?\n",
      "1st Closest document: Get Set 4 Swimming - H2OMG! Can this man swim around Britain? Score: 0.8159745583466792\n",
      "2nd Closest document: Ross Edgley, at 33 - first man to swim around Britain Score: 0.7678877104085525\n"
     ]
    }
   ],
   "source": [
    "def get_closest_documents(query, vectorizer, train_corpus_vectors, top_n=2):\n",
    "    \"\"\"Vectorizer should be fit on the documents beforehand.\n",
    "        Returns tuples of (similarity, indexes) of closest documents\"\"\"\n",
    "    \n",
    "\n",
    "train_corpus_vectors = tfidf_vectorizer.transform(documents)\n",
    "closest_documents = get_closest_documents(query, tfidf_vectorizer, train_corpus_vectors)\n",
    "print('Query:', query)\n",
    "print('1st Closest document: {} Score: {}'.format(documents[closest_documents[0][1]], closest_documents[0][0]))\n",
    "print('2nd Closest document: {} Score: {}'.format(documents[closest_documents[1][1]], closest_documents[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: using the friends corpus try to create an IR chatbot:\n",
    "- User writes a sentences and we find the __closest sentence__ from the transcript\n",
    "- We need to take __the answer__ to that sentence to make a dialogue! \n",
    "- Provide the bot with a __personality__, selecting only the tuple cues, where the answer is by a specific person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didimitrov\\AppData\\Local\\Temp\\ipykernel_19692\\454469593.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>person</th>\n",
       "      <th>gender</th>\n",
       "      <th>original_line</th>\n",
       "      <th>line</th>\n",
       "      <th>metadata</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>496501</td>\n",
       "      <td>266</td>\n",
       "      <td>RACHEL</td>\n",
       "      <td>F</td>\n",
       "      <td>Rachel: Let's just say my Curious George doll is no longer curious.</td>\n",
       "      <td>Let's just say my Curious George doll is no longer curious.</td>\n",
       "      <td>Let_VM21 's_VM22 just_RR say_VVI my_APPGE Curious_JJ George_NP1 doll_NN1 is_VBZ no_RR21 longer_RR22 curious_JJ ._.</td>\n",
       "      <td>0121.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>47001</td>\n",
       "      <td>23</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?</td>\n",
       "      <td>Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?</td>\n",
       "      <td>Well_RR uh_UH uh_UH I_PPIS1 do_VD0 n't_XX know_VVI okay_RR okay_RR how_RRQ about_II with_IW the_AT uh_UH with_IW the_AT baby_NN1 's_GE name_NN1 ?_?</td>\n",
       "      <td>0102.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28980</th>\n",
       "      <td>2894901</td>\n",
       "      <td>1489</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: Hm-mmm.</td>\n",
       "      <td>Hmmmm.</td>\n",
       "      <td>Hm-mmm._NNU</td>\n",
       "      <td>0519.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22389</th>\n",
       "      <td>2235801</td>\n",
       "      <td>1147</td>\n",
       "      <td>RACHEL</td>\n",
       "      <td>F</td>\n",
       "      <td>Rachel: Relaxi-Taxi!</td>\n",
       "      <td>Relaxi-Taxi!</td>\n",
       "      <td>Relaxi-Taxi_NP1 !_!</td>\n",
       "      <td>0417.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59246</th>\n",
       "      <td>5921501</td>\n",
       "      <td>2968</td>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoebe: Noooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.</td>\n",
       "      <td>Nooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.</td>\n",
       "      <td>Nooo_NN1 !_! Ok_RR maybe_RR if_CS we_PPIS2 just_RR break_VV0 it_PPH1 down_RP ._. Ok_RR let_VM21 's_VM22 try_VVI at_II one_MC1 syllable_NN1 at_II a_AT1 time_NNT1 ._. Ok_RR ?_? So_RR repeat_VV0 after_II me._NNU je_FW ._.</td>\n",
       "      <td>1013.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id scene_id  person gender  \\\n",
       "4974   496501   266      RACHEL  F       \n",
       "470    47001    23       ROSS    M       \n",
       "28980  2894901  1489     ROSS    M       \n",
       "22389  2235801  1147     RACHEL  F       \n",
       "59246  5921501  2968     PHOEBE  F       \n",
       "\n",
       "                                                                                                                 original_line  \\\n",
       "4974   Rachel: Let's just say my Curious George doll is no longer curious.                                                       \n",
       "470    Ross: Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?                               \n",
       "28980  Ross: Hm-mmm.                                                                                                             \n",
       "22389  Rachel: Relaxi-Taxi!                                                                                                      \n",
       "59246  Phoebe: Noooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.   \n",
       "\n",
       "                                                                                                                 line  \\\n",
       "4974   Let's just say my Curious George doll is no longer curious.                                                      \n",
       "470    Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?                            \n",
       "28980  Hmmmm.                                                                                                           \n",
       "22389  Relaxi-Taxi!                                                                                                     \n",
       "59246  Nooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.   \n",
       "\n",
       "                                                                                                                                                                                                                         metadata  \\\n",
       "4974   Let_VM21 's_VM22 just_RR say_VVI my_APPGE Curious_JJ George_NP1 doll_NN1 is_VBZ no_RR21 longer_RR22 curious_JJ ._.                                                                                                           \n",
       "470    Well_RR uh_UH uh_UH I_PPIS1 do_VD0 n't_XX know_VVI okay_RR okay_RR how_RRQ about_II with_IW the_AT uh_UH with_IW the_AT baby_NN1 's_GE name_NN1 ?_?                                                                          \n",
       "28980  Hm-mmm._NNU                                                                                                                                                                                                                  \n",
       "22389  Relaxi-Taxi_NP1 !_!                                                                                                                                                                                                          \n",
       "59246  Nooo_NN1 !_! Ok_RR maybe_RR if_CS we_PPIS2 just_RR break_VV0 it_PPH1 down_RP ._. Ok_RR let_VM21 's_VM22 try_VVI at_II one_MC1 syllable_NN1 at_II a_AT1 time_NNT1 ._. Ok_RR ?_? So_RR repeat_VV0 after_II me._NNU je_FW ._.   \n",
       "\n",
       "       filename  \n",
       "4974   0121.txt  \n",
       "470    0102.txt  \n",
       "28980  0519.txt  \n",
       "22389  0417.txt  \n",
       "59246  1013.txt  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "friends_corpus = pd.read_csv(\"data/friends-final.txt\", sep='\\t')\n",
    "friends_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>There's nothing to tell! He's just some guy I work with!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOEY</td>\n",
       "      <td>C'mon, you're going out with the guy! There's gotta be something wrong with him!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Just, 'cause, I don't want her to go through what I went through with Carl- oh!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Then I look down, and I realize there's a phone... there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person  \\\n",
       "0  MONICA     \n",
       "1  JOEY       \n",
       "2  CHANDLER   \n",
       "3  PHOEBE     \n",
       "4  PHOEBE     \n",
       "5  MONICA     \n",
       "6  CHANDLER   \n",
       "7  CHANDLER   \n",
       "8  ALL        \n",
       "9  CHANDLER   \n",
       "\n",
       "                                                                                                                  line  \n",
       "0  There's nothing to tell! He's just some guy I work with!                                                             \n",
       "1  C'mon, you're going out with the guy! There's gotta be something wrong with him!                                     \n",
       "2  Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?                                               \n",
       "3  Wait, does he eat chalk?                                                                                             \n",
       "4  Just, 'cause, I don't want her to go through what I went through with Carl- oh!                                      \n",
       "5  Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.         \n",
       "6  Sounds like a date to me.                                                                                            \n",
       "7  Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.  \n",
       "8  Oh, yeah. Had that dream.                                                                                            \n",
       "9  Then I look down, and I realize there's a phone... there.                                                            "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example conversation\n",
    "friends_corpus[friends_corpus['scene_id']=='1'][['person', 'line']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60849, 15032)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer().fit(friends_corpus.line)\n",
    "train_corpus = vectorizer.transform(friends_corpus.line)\n",
    "train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence:  C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Its vector representation:  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "An id of a word from the sentence:  14756\n",
      "The word tf-idf score:  0.41270625402865396\n"
     ]
    }
   ],
   "source": [
    "print(\"First sentence: \", friends_corpus.line.values[1])\n",
    "print(\"Its vector representation: \", train_corpus[1].toarray())\n",
    "print(\"An id of a word from the sentence: \", vectorizer.vocabulary_['with'])\n",
    "print(\"The word tf-idf score: \", train_corpus[1].toarray()[0][vectorizer.vocabulary_['with']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8125, 5411)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the previous line, which the cue follows in the dialogue\n",
    "friends_corpus['previous_line'] = ['DUMMY PREVIOUS LINE'] + friends_corpus['line'].values[:-1].tolist()\n",
    "# select only the cues which are made by JOEY\n",
    "joey_line_tuples = friends_corpus[friends_corpus.person == 'JOEY']\n",
    "# create a vectorizer and training space of the documents in the vector space\n",
    "joey_vectorizer = TfidfVectorizer().fit(joey_line_tuples.previous_line)\n",
    "joey_train_corpus = joey_vectorizer.transform(joey_line_tuples.previous_line)\n",
    "joey_train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_utterance(cue, vectorizer,  train_corpus, top_n=5):\n",
    "    # compute similarity to all sentences in the training corpus\n",
    "    similarities = cosine_similarity(vectorizer.transform([cue]), train_corpus).flatten()\n",
    "    # get indexes of top 5 clocest sentences\n",
    "    related_docs_indices = similarities.argsort()[:-top_n:-1]\n",
    "    # return tuples of (similarity score, sentence)\n",
    "    return [(similarities[idx], joey_line_tuples['line'].values[idx]) \n",
    "                for idx in related_docs_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  \"Joey Tribbiani! From the wall! Okay, maybe this will jog your memory, huh? Huh? Okay eh-ah-anyway, I'm ready to go back up on the wall I'm the star of a new TV show.\"),\n",
       " (1.0,\n",
       "  \"Oh, hi, I'm Joey. My stupid friends are buying this house. Who are you?\"),\n",
       " (0.7527478944848099, 'Me.'),\n",
       " (0.7527478944848099, \"Alright. I'll give you one hint. Warren Beatty.\")]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_utterance('who are you?', joey_vectorizer, joey_train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
